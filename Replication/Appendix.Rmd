uploading the librarires

```{r}
library(haven)
library(MASS)
# library(huxtable)
library(texreg)
library(sandwich)
library(lmtest)
library(caret)
library(cvTools) # For k-fold CV
library(Metrics) # For error metrics

```

Upload the data

```{r}
#library(margins)
#library(ggplot2)
dat <- read_dta('bbdm_drug_war.dta')
dat <- na.omit(dat)
dat$level_edu <- 100 - dat$pct_hs_grad
# any(is.na(dat))
```

```{r}
hist(dat$killing_count, 
     main = "Distribution of the count of killings", 
     xlab = "Killing Count", 
     xlim = c(0,30), 
     breaks = 100)
```

Replicaitng model 4 (negative binomial)

```{r}
#Dependent variable 
#the dependent variable is the total count of killings 
dat$killing_count = dat$police_killing_count + dat$vigilante_killing_count 
# sum(dat$killing_count)
# names(dat)


model_repli <- glm.nb(killing_count ~ rc_dummy + pct_catholic + pct_hs_grad + 
                 pct_at_risk_pop + duterte_share + enp_sen + police_dummy + 
                 pct_nccp + methodist_dummy + offset(log(population)), #log population automatically applied on stata 
                 data = dat)

coeftest(model_repli, vcov = vcovCL(model_repli, cluster = ~ municipality))



#taking the exponents of the coefficients because Stata uses exponentiated coefficients while R reports log coefficients 
model_repli_exp <- exp(coef(model_repli))

#making the table 
screenreg(
  model_repli,
  override.coef = list(model_repli_exp), 
  custom.model.names = "Replicated Model", 
  include.aic = TRUE, 
  include.bic = TRUE, 
  include.loglik = TRUE,  
  stars = c(0.1, 0.05, 0.01)) #to transform the estimates in exponents 

```

I want to fit another model that focuses more on economic uncertainty and ending education, so I will run a model with the interaction of those two.

```{r}
#Fitting a model that considers if the interaction between not getting a diploma and unemployment (economic uncertainty) affects the number of killings

model_polecon <- glm.nb(killing_count ~ rc_dummy + pct_catholic + 
                            level_edu * pct_informal_unemp + 
                            pct_at_risk_pop + duterte_share + enp_sen + 
                            police_dummy + pct_nccp + methodist_dummy + 
                            offset(log(population)), data = dat)

model_polecon_summary <- summary(model_polecon)

#taking the exponents of the coefficients because Stata uses exponentiated coefficients while R reports log coefficients 
model_polecon_summary$coefficients[, 1] <- exp(model_polecon_summary$coefficients[, 1])
print(model_polecon_summary)


#Fitting a model that takes into account the interaction between unemployment and high school diploma, but that does not focus a lot on the political environment 

model_econ <- glm.nb(killing_count ~ rc_dummy + pct_catholic + 
                            level_edu * pct_informal_unemp + 
                            pct_at_risk_pop + police_dummy + pct_nccp 
                            + methodist_dummy + offset(log(population)), data = dat)

model_econ_summary <- summary(model_econ)

#taking the exponents of the coefficients because Stata uses exponentiated coefficients while R reports log coefficients 
model_econ_summary$coefficients[, 1] <- exp(model_econ_summary$coefficients[, 1])
print(model_econ_summary)

#the latter model gives more importance to economic factors rather than political ones. In this way, we can see if economic factors by themselves still predict killings. The economic conditions have an independent effect even not accounting for political factors. This is confirmed by the fact that the predictors are significant. 

```

Creating a table with all the models

```{r}
model_repli_exp <- exp(coef(model_repli))
model_polecon_exp <- exp(coef(model_polecon))
model_econ_exp <- exp(coef(model_econ))

screenreg(list(model_repli, model_polecon, model_econ),
          override.coef = list(model_repli_exp, model_polecon_exp, model_econ_exp), 
          custom.model.names = c("Replicated Model", "Economic and Political Model", "Economic Model"), 
          # custom.coef.names = c(""), 
          include.aic = TRUE, include.bic = TRUE, include.loglik = TRUE, 
          stars = c(0.1, 0.05, 0.01), 
          transform = exp) #to transform the estimates in exponents 
```

In sample evaluation. The models present quite the same standard errors, meaning none is better. The AIC is lowest for the Economic Model as well as the BIC, meaning that it performs better in sample. Regarding the other models, one performs better for the AIC (Economic and Political) while the other in the BIC (the replicated model). Even if the Economic Model cannot really considered as simpler than the replicated one, it is simpler than the Economic and Political Model (neastead), and it is perfomrming better, meaning that it would be a better choice compared to the Economic and Political one. The deviance is better for the Economic model as well, showing that it is perfomring better based on in sample evaluation. However, the difference is not a lot and out of sample evaluation is needed to understand which model performs better.

Trying the visualization for in sample

```{r}
library(ggplot2)
library(dplyr)
library(tidyr)
library(MASS)  # For glm.nb

# Add predictions from all models
dat <- dat %>%
  mutate(pred_repli = predict(model_repli, newdata = ., type = "response"),
         pred_polecon = predict(model_polecon, newdata = ., type = "response"),
         pred_econ = predict(model_econ, newdata = ., type = "response"))

# Reshape data to long format for easier ggplot handling
dat_long <- dat %>%
  pivot_longer(cols = starts_with("pred_"), names_to = "model", values_to = "predicted")

# Rename model labels for readability
dat_long$model <- recode(dat_long$model, 
                         "pred_repli" = "Replication Model",
                         "pred_polecon" = "Political Economy Model",
                         "pred_econ" = "Economic Model")

min_val <- min(c(dat_long$predicted, dat$killing_count), na.rm = TRUE)
max_val <- max(c(dat_long$predicted, dat$killing_count), na.rm = TRUE)


# Plot
ggplot(dat_long, aes(x = predicted, y = killing_count, color = model)) +
  geom_point(alpha = 0.6) +  # Scatter plot for all models
  geom_line(data = data.frame(x = c(min_val, max_val), y = c(min_val, max_val)), 
            aes(x = x, y = y), linetype = "dotted", color = "black") +  
  labs(x = expression("Predicted values: " * hat(Y)), 
       y = expression("Real values: " * Y), 
       title = "Comparison of Model Predictions") +
  theme_minimal() +
  scale_color_manual(values = c("pink", "yellow", "purple")) 

```

Creating a ROC curve by splitting the predictions in binary data and see the performance of each model.

```{r}
library(pROC)
library(ggplot2)

#making binary outcome 
dat$binary_outcome <- ifelse(dat$killing_count > 0, 1, 0)

#taking the predictions for different models 
dat$pred_replica <- predict(model_repli, type = "response")
dat$pred_polecon <- predict(model_polecon, type = "response")
dat$pred_econ <- predict(model_econ, type = "response")

#creating the curves 
curve_replica <- roc(dat$binary_outcome, dat$pred_replica)
curve_polecon <- roc(dat$binary_outcome, dat$pred_polecon)
curve_econ <- roc(dat$binary_outcome, dat$pred_econ)


plot(curve_replica, main = "In sample evaluation", bty = "n", lwd = 2, col = "purple")
plot(curve_polecon, lwd = 2, col = "yellow", add = T)
plot(curve_econ, lwd = 2, col = "pink", add = T)

#legend 
legend("bottomright", legend = c("Replication Model", "Poltical and Economic Model", "Economic Model"), 
       col = c("purple", "yellow", "pink"), lty = 1)

```

Testing the hurdle model. The negative binomial is performing better, so I will keep that one, but need to ask (maybe add to the appendix).

```{r}
library(pscl)
hurdle_polecon <- hurdle(killing_count ~ rc_dummy + pct_catholic + 
                            level_edu * pct_informal_unemp + 
                            pct_at_risk_pop + duterte_share + enp_sen + 
                            police_dummy + pct_nccp + methodist_dummy + 
                            offset(log(population)), data = dat, dist = "negbin")
summary(hurdle_polecon)
AIC(hurdle_polecon)
BIC(hurdle_polecon)

```

```{r}
library(car)
# Compute VIF
vif_values <- vif(model_econ)

# Print VIF values
print(vif_values)
```

Out of sample validation

```{r}

# Set number of folds
k <- 10
set.seed(123)

# Create k folds
folds <- cvFolds(nrow(dat), K = k)

# Function to perform k-fold CV and compute metrics
cv_nb_model <- function(formula, data, folds) {
  mse_values <- numeric(k)
  rmse_values <- numeric(k)
  mae_values <- numeric(k)
  r2_values <- numeric(k)
  pearson_corr <- numeric(k)
  mape_values <- numeric(k)
  
  for (i in 1:k) {
    # Split data into train and test sets
    train_idx <- which(folds$which != i)
    test_idx <- which(folds$which == i)
    
    train_data <- data[train_idx, ]
    test_data <- data[test_idx, ]
    
    # Fit negative binomial model
    model <- glm.nb(formula, data = train_data)
    
    # Predict on test set
    preds <- predict(model, newdata = test_data, type = "response")
    
    # Compute Metrics
    mse_values[i] <- mean((test_data$killing_count - preds)^2)  # MSE
    rmse_values[i] <- sqrt(mse_values[i])                      # RMSE
    mae_values[i] <- mean(abs(test_data$killing_count - preds)) # MAE
    r2_values[i] <- cor(test_data$killing_count, preds)^2      # R-squared
    pearson_corr[i] <- cor(test_data$killing_count, preds)     # Pearson Correlation
    mape_values[i] <- mean(abs((test_data$killing_count - preds) / test_data$killing_count)) * 100  # MAPE
  }
  
  # Return aggregated metrics
  return(c(
    mean(mse_values), mean(rmse_values), mean(mae_values),
    mean(r2_values), mean(pearson_corr), mean(mape_values)
  ))
}

# Define formulas for models
formula_replica <- killing_count ~ rc_dummy + pct_catholic + pct_hs_grad + 
                pct_at_risk_pop + duterte_share + enp_sen + police_dummy + 
                pct_nccp + methodist_dummy + offset(log(population))

formula_polecon <- killing_count ~ rc_dummy + pct_catholic + 
                   level_edu * pct_informal_unemp + pct_at_risk_pop + 
                   duterte_share + enp_sen + police_dummy + pct_nccp + 
                   methodist_dummy + offset(log(population))

formula_econ <- killing_count ~ rc_dummy + pct_catholic + 
                level_edu * pct_informal_unemp + pct_at_risk_pop + 
                police_dummy + pct_nccp + methodist_dummy + offset(log(population))

# Run cross-validation for each model
results_replica <- cv_nb_model(formula_replica, dat, folds)
results_polecon <- cv_nb_model(formula_polecon, dat, folds)
results_econ <- cv_nb_model(formula_econ, dat, folds)

# Combine results into a dataframe
cv_results <- data.frame(
  Model = c("Replica Model", "Political Economy Model", "Economic Model"),
  MSE = c(results_replica[1], results_polecon[1], results_econ[1]),
  RMSE = c(results_replica[2], results_polecon[2], results_econ[2]),
  MAE = c(results_replica[3], results_polecon[3], results_econ[3]),
  R_squared = c(results_replica[4], results_polecon[4], results_econ[4]),
  Pearson_Correlation = c(results_replica[5], results_polecon[5], results_econ[5]),
  MAPE = c(results_replica[6], results_polecon[6], results_econ[6])
)

# Print results
print(cv_results)

```

```{r}
#plotting the MSE 
colors <- c("purple", "yellow", "pink")

barplot(cv_results$MSE, names.arg = cv_results$Model, xlab = "Models",
        col = colors, 
        ylab = "MSE", main = "MSE of Models", 
        ylim = c(0, 10), 
        yaxt = "n")

axis(2, at = seq(0, 10, by = 1))

```

```{r}
#plotting the R squared 

barplot(cv_results$R_squared, names.arg = cv_results$Model, xlab = "Models",
        col = colors, 
        ylab = "MSE", main = "R Squared of Models", 
        ylim = c(0, 1), 
        yaxt = "n")

axis(2, at = seq(0, 1, by = 0.05))
```

Trying the ROC curve here as well.

```{r}
# Set number of folds and seed
k <- 10
set.seed(123)

# Create k folds for cross-validation
folds <- cvFolds(nrow(dat), K = k)

# Cross-validation function: Computes metrics and stores ROC data
cv_nb_model_with_metrics <- function(formula, data, folds) {
  mse_values <- numeric(k)
  rmse_values <- numeric(k)
  mae_values <- numeric(k)
  r2_values <- numeric(k)
  pearson_corr <- numeric(k)
  mape_values <- numeric(k)
  
  actual_values <- c()  # Store actual (binary)
  predicted_probs <- c()  # Store predicted probabilities

  for (i in 1:k) {
    # Split data into train and test sets
    train_idx <- which(folds$which != i)
    test_idx <- which(folds$which == i)
    
    train_data <- data[train_idx, ]
    test_data <- data[test_idx, ]
    
    # Fit negative binomial model
    model <- glm.nb(formula, data = train_data)
    
    # Predict on test set
    preds <- predict(model, newdata = test_data, type = "response")
    
    # Store binary actual values (1 = killings occurred, 0 = no killings)
    actual_values <- c(actual_values, ifelse(test_data$killing_count > 0, 1, 0))  
    predicted_probs <- c(predicted_probs, preds)  

    # Compute performance metrics
    mse_values[i] <- mean((test_data$killing_count - preds)^2)  # MSE
    rmse_values[i] <- sqrt(mse_values[i])  # RMSE
    mae_values[i] <- mean(abs(test_data$killing_count - preds))  # MAE
    r2_values[i] <- cor(test_data$killing_count, preds)^2  # RÂ²
    pearson_corr[i] <- cor(test_data$killing_count, preds)  # Pearson Correlation
    mape_values[i] <- mean(abs((test_data$killing_count - preds) / test_data$killing_count)) * 100  # MAPE
  }
  
  # Return performance metrics and ROC-ready data
  return(list(
    metrics = c(mean(mse_values), mean(rmse_values), mean(mae_values),
                mean(r2_values), mean(pearson_corr), mean(mape_values)),
    roc_data = data.frame(Actual = actual_values, Predicted_Prob = predicted_probs)
  ))
}

# Define formulas for three models
formula_full <- killing_count ~ rc_dummy + pct_catholic + pct_hs_grad + 
                pct_at_risk_pop + duterte_share + enp_sen + police_dummy + 
                pct_nccp + methodist_dummy + offset(log(population))

formula_polecon <- killing_count ~ rc_dummy + pct_catholic + 
                   level_edu * pct_informal_unemp + pct_at_risk_pop + 
                   duterte_share + enp_sen + police_dummy + pct_nccp + 
                   methodist_dummy + offset(log(population))

formula_econ <- killing_count ~ rc_dummy + pct_catholic + 
                level_edu * pct_informal_unemp + pct_at_risk_pop + 
                police_dummy + pct_nccp + methodist_dummy + offset(log(population))

# Run cross-validation for each model
results_full <- cv_nb_model_with_metrics(formula_full, dat, folds)
results_polecon <- cv_nb_model_with_metrics(formula_polecon, dat, folds)
results_econ <- cv_nb_model_with_metrics(formula_econ, dat, folds)

# Extract performance metrics for each model
cv_results <- data.frame(
  Model = c("Full Model", "Political Economy Model", "Economic Model"),
  MSE = c(results_full$metrics[1], results_polecon$metrics[1], results_econ$metrics[1]),
  RMSE = c(results_full$metrics[2], results_polecon$metrics[2], results_econ$metrics[2]),
  MAE = c(results_full$metrics[3], results_polecon$metrics[3], results_econ$metrics[3]),
  R_squared = c(results_full$metrics[4], results_polecon$metrics[4], results_econ$metrics[4]),
  Pearson_Correlation = c(results_full$metrics[5], results_polecon$metrics[5], results_econ$metrics[5]),
  MAPE = c(results_full$metrics[6], results_polecon$metrics[6], results_econ$metrics[6])
)

# Print performance results
print(cv_results)

# Compute ROC curves using out-of-sample predictions
roc_full <- roc(results_full$roc_data$Actual, results_full$roc_data$Predicted_Prob)
roc_polecon <- roc(results_polecon$roc_data$Actual, results_polecon$roc_data$Predicted_Prob)
roc_econ <- roc(results_econ$roc_data$Actual, results_econ$roc_data$Predicted_Prob)

# Plot ROC curves using base R
plot(roc_full, col = "blue", main = "Cross-Validated ROC Curves", lwd = 2)
lines(roc_polecon, col = "red", lwd = 2)
lines(roc_econ, col = "green", lwd = 2)

legend("bottomright", legend = c("Full Model", "Political Economy Model", "Economic Model"),
       col = c("blue", "red", "green"), lwd = 2)


```

Take a variable of choice. Since the main objective is to understand the influence of international organizations on the killings, I will focus on the presence of Catholic parishes. I am focusing on that because it is an example of an organization that has roots in the social environment in the Philippines. I picked the model that performs better, which is the model that focuses on economic factors.

```{r}
library(ggplot2)
library(dplyr)

# Define predictor values for the two scenarios
X.presence <- data.frame(
  rc_dummy = 1, 
  pct_catholic = mean(dat$pct_catholic, na.rm = TRUE), 
  level_edu = mean(dat$level_edu, na.rm = TRUE), 
  pct_informal_unemp = mean(dat$pct_informal_unemp, na.rm = TRUE), 
  pct_at_risk_pop = mean(dat$pct_at_risk_pop, na.rm = TRUE), 
  police_dummy = median(dat$police_dummy, na.rm = TRUE), 
  pct_nccp = mean(dat$pct_nccp, na.rm = TRUE), 
  methodist_dummy = median(dat$methodist_dummy, na.rm = TRUE), 
  interaction = mean(dat$level_edu, na.rm = TRUE) * mean(dat$pct_informal_unemp, na.rm = TRUE),
  population = mean(dat$population, na.rm = TRUE) # Offset variable
)

X.absence <- data.frame(
  rc_dummy = 0, 
  pct_catholic = mean(dat$pct_catholic, na.rm = TRUE), 
  level_edu = mean(dat$level_edu, na.rm = TRUE), 
  pct_informal_unemp = mean(dat$pct_informal_unemp, na.rm = TRUE), 
  pct_at_risk_pop = mean(dat$pct_at_risk_pop, na.rm = TRUE), 
  police_dummy = median(dat$police_dummy, na.rm = TRUE), 
  pct_nccp = mean(dat$pct_nccp, na.rm = TRUE), 
  methodist_dummy = median(dat$methodist_dummy, na.rm = TRUE), 
  interaction = mean(dat$level_edu, na.rm = TRUE) * mean(dat$pct_informal_unemp, na.rm = TRUE),
  population = mean(dat$population, na.rm = TRUE) # Offset variable
)

# Predict killing counts for both scenarios using the Negative Binomial model
X.presence$predicted <- predict(model_econ, newdata = X.presence, type = "response")
X.absence$predicted <- predict(model_econ, newdata = X.absence, type = "response")

# Combine data for visualization
plot_data <- bind_rows(
  X.presence %>% mutate(rc_dummy = "Parish Present"),
  X.absence %>% mutate(rc_dummy = "Parish Absent")
) %>%
  select(rc_dummy, predicted)

# Compute Standard Error for Error Bars
se <- sd(dat$killing_count, na.rm = TRUE) / sqrt(nrow(dat))  # Approximate SE

# Create visualization
ggplot(plot_data, aes(x = rc_dummy, y = predicted)) +
  geom_point(color = "blue", size = 3) +  # Points for predicted values
  geom_errorbar(aes(ymin = predicted - 1.96 * se, ymax = predicted + 1.96 * se), 
                width = 0.2, color = "red") +  # Confidence intervals
  labs(title = "Effect of Parish Churches on Predicted Killings",
       x = "Presence of Parish Churches",
       y = "Predicted Killing Counts") +
  theme_minimal()

``` 